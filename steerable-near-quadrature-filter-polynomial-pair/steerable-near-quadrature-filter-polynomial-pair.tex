\documentclass{article}


\usepackage{arxiv} % for arxiv template format a

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{verbatim} %For \comment environment

\usepackage[english]{babel}  % english
\usepackage{amsmath,amsthm,amssymb}% for theorems
% THEOREMS -------------------------------------------------------
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}	
\newtheorem{prob}[thm]{Problem}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{ex}{Example}[section]
\numberwithin{equation}{section}
% ----------------------------------------------------------------

%Just some declaremathoperators
\DeclareMathOperator{\Rad}{Rad}
\DeclareMathOperator{\BetaDistr}{Beta}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\mode}{mode}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\Dir}{Dir}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\characteristic}{char}


\title{On quasi-conjugate polynomial pairs for three-dimensional steerable near-quadrature filters}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{
 Tommy M. Tang  \\
  Department of Radiology and Biomedical Imaging\\
  Yale University\\
   300 Cedar St, New Haven\\
  New Haven, 06510 \\
  \texttt{tommymtang@gmail.com} \\
  %% examples of more authors
  \And 
  Jose L. Vilas-Prieto\\
   Department of Radiology and Biomedical Imaging\\
  Yale University\\
   300 Cedar St, New Haven\\
  New Haven, 06510 \\
  \texttt{joseluis.vilas-prieto@yale.edu} \\
   \And
 Hemant D. Tagare \thanks{also Department of Biomedical Engineering, Electrical Engineering, Statistics and Data Science} \\
 Department of Radiology and Biomedical Imaging\\
 Yale University\\
 300 Cedar St, New Haven\\
  New Haven, 06510 \\
  \texttt{hemant.tagare@yale.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}

\begin{document}
\maketitle

\begin{abstract}
For the construction of axially symmetric three-dimensional filters pairs to approximately represent the real and imaginary parts of a quadrature filter - giving the analytic representation of the impulse response of a filter - it is necessary to construct a pair of polynomials - one odd, one even - that differ minimally on the unit interval. This minimization is convex and therefore straightforward to implement. However, it is not clear that such a pair of polynomials would necessarily exhibit other desirable properties of this minimization, such as positivity, concentration at one, or that this difference would vanish as we allow our maximum bandwidth N to increase. We demonstrate that under the constraints that the polynomials vanish at the origin and are normalized so that the coefficients add up to one, we may in fact guarantee (1) the optimally similar polynomial pairs have positive coefficients, (2) the objective function loss vanishes as the maximum degree N allowed approaches infinity and (3) these polynomials converge pointwise to the indicator function at 1 on the relevant unit interval. In the process, we explore various properties of the closely related Hilbert matrices and their inversion. 
\end{abstract}


% keywords can be removed
\keywords{steerability \and quadrature \and filter \and quadratic programming \and hilbert matrices}


\section{Introduction}
\label{Introduction}
In this section we give the application motives for our results. We introduce the need to approximately represent quadrature filters using near-conjugate filters and explore briefly certain sufficient conditions for steerability. We precisely define our optimization problem in terms of an odd and even polynomial pair under a bandwidth N, and finally reformulate our problem in vectorized form using generalized Hilbert matrices, the mode of the problem we will primarily work in.
\subsection{Motivation}
Analytic signals provide a powerful framework for the extraction of local properties of signals. Quadrature filters that give Hilbert transforms of signals are essential to these calculations, and several methods have been proposed to extend the Hilbert transform to higher dimensions \cite{boukerroui2004choice}. Of these, methods for two-dimensional signal via the use of steerable filters to combine information about images at various orientations have been proposed to preserve the isotropy property, necessary to obtain invariance of the measure of the impulse of the local signal under rotation. \\
However, three-dimensional filters remain technically difficult. Since quadrature filters are often represented as two real filters - it is invaluable to be able to construct two filters that are harmonic conjugates of each other. In a three-dimensional sense, these filters must agree on a certain domain and differ by only a sign on the complement. Additionally, it would be computationally desirable to demand that such filter pairs are steerable. \\
A fast method of computing steering coefficients of axially symmetric functions is known \cite{derpanis2005three}. We therefore desire a pair of axially symmetric functions $f_e, f_o$ such that $f_e$ is symmetric over the $xy$-plane and $f_o$ is anti-symmetric over the $xy$-plane, and such that $|f_e-f_o|$ is close to 0 on the upper hemisphere. In particular, these functions are radial-angular separable and their sign depends only on the angular part, which will in fact be a polynomial of $\cos \theta$. \\
Thus it is natural to attempt to find pairs of polynomials $p_e, p_o$ such that on the interval $[0,1]$, $(p_e-p_o)^2$ is minimized, and such that $p_e(0)=p_o(0)=0, p_e(1)=p_o(1)=1$. This problem can be framed as a quadratic programming problem and as it involves only convex optimization, has a guaranteed optimum. However, it is not obvious what other properties of the polynomials would be guaranteed. We do not know, for example, if the polynomials will be guaranteed to be monotonic or even positive, two essential properties for constructing useful filters that would approximate, for example, a Gabor transform. Furthermore, it is unclear how the polynomial solutions would behave as we allow the bandwidth to go to infinity: would the error in harmonic conjugacy - the difference $(p_e-p_o)^2$ indeed vanish? Do the polynomials converge pointwise to functions concentrated at the endpoint 1? \\
In this manuscript, we show that our optimization problem is intimately connected to the inverse of generalized Hilbert matrices. Using well-known and newly proven properties of these Hilbert matrices, we prove desirable qualities of the polynomial pairs that can be used to construct three-dimensional analogues of steerable filter pairs in quadrature. 

\subsection{The quadratic optimization problem}
\label{problem}

We now formulate the problem precisely. We are interested in quasiconjugate, radial-angular separable, exactly steerable functions of $\mathbb{R}^3$. That is, we consider functions of the type $f_e = W(r)p_e(\cos \theta)$ and $f_o = W(r)p_o(\cos \theta)$ where $p_e, p_o$ are axially symmetric about the north pole and positive and approximately identical on the upper half sphere. \\

We can think of $p_e, p_o$ as functions of $K/G/K$ where $G=SO(3)$ and $K$ is an embedding of $SO(2)$. In particular, they are functions of the altitude on the sphere and we can thus consider them as functions of $\cos \theta$, ranging from $-1$ to $1$. When $p_e, p_o$ are polynomials, the functions $f_e, f_o$ are steerable with respect to $SO(3)$ and moreover the steering coefficients are quickly computed. Thus it is possible to compute the integral of a convolution with these functions over different orientations in a closed form.

We desire that both $p_e$ and $p_o$ are zero at the equator and 1 at the poles. We hope to show that if we simply minimize the integral of their difference over the upper half sphere, we may be guaranteed solutions that exhibit certain desirable properties such as positivity, monotonicity, and concentration. \\

Let us fix a maximum bandwidth $N$ - this is the maximum degree allowed in our polynomials. We will minimize the integral
\begin{align}
&\int_0^{\pi/2} (p_e(\cos\theta) - p_o(\cos\theta))^2 \sin\theta d\theta\\ \label{quasiconjugacy-poly-formulation}
&= \int_0^1 (p_e(z) - p_o(z))^2 dz
\end{align}
under the constraint that $p_e(0)=p_o(0) = 0$ and $p_e(1) = p_o(1) = 1$. The integral in (\ref{quasiconjugacy-poly-formulation}) can be computed more explicitly and written in a vectorized form. \\
Let $p(z) = a_Nz^N + a_{N-1}z^{N-1} + ... + a_0 = p_e(z) - p_o(z)$. Note that the coefficients of $p$ correspond in a straightforward way to $p_e$, $p_o$, with the mere modification of sign in $p_o$. We require $p(0) = 0$, $p(1) = 1$. Note that $p(0)=0$ implies that $a_0 = 0$. Therefore, we may let  $a\in \mathbb{R}^N$ with $a(i) = a_i$ in $p$. Then our problem can be rewritten as: 

\begin{align*}
& \argmin_{a\in\mathbb{R}^N p(0)=0,p(1)=1} \int_0^1 p(z)^2 dz, \qquad p(0)=0, p(1)=1 \\
&=\argmin_{a\in\mathbb{R}^N} \int_0^1 \sum_{i,j=1}^N a_ia_j z^{i+j} dz, \qquad p(0)=0, p(1)=1\\
&= \argmin_{a\in\mathbb{R}^N} \sum_{i,j=1}^N a_ia_j \int_0^1 z^{i+j} dz, \qquad p(0)=0, p(1)=1\\
&= \argmin_{a\in\mathbb{R}^N} \sum_{i,j=1}^N a_ia_j \frac{1}{i+j+1} dz, \qquad p(0)=0, p(1)=1\\
&= \argmin_{a\in\mathbb{R}^N} a^T H a, \qquad \sum_{i \;\text{odd}} a(i) = -1, \sum_{i \;\text{even}} a(i) = 1
\end{align*} where $H_{ij} = \frac{1}{i+j+1}$. \\

\subsection{Reformulation with Generalized Hilbert Matrix}
\label{reformulation-hilbert}
In the last line of our rewriting in Section \ref{problem}, we saw that it was possible to rewrite our integral problem as a quadratic optimization problem, using a matrix $H$. We will see that this $H$ is a special case of a \emph{generalized Hilbert matrix}, which we define below. We will see additional properties of these matrices in Section \ref{hilbert-matrices}. Here we will merely rewrite our problem in standard quadratic programming notation - we will adopt this philosophy towards the problem for the remainder of this manuscript. 
\begin{defn}\label{def-generalized-hilbert-matrix}
Let $\mathbb{Z} \ni p \ge 0$. A \emph{generalized Hilbert matrix} is a matrix $H\in M_N(\mathbb{R})$ whose entries are defined: 
$H_{ij} = \frac{1}{i+j-1+p}$
\end{defn}
We now reformulate our problem. 
\begin{prob}\label{quad-prog-problem}
Let $H$ be a generalized Hilbert matrix of size $N$, and $B = \begin{pmatrix} 1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 1\end{pmatrix} ^T \in M_{N\times 2}(\mathbb{R}$). We must solve the quadratic programming problem: 
\begin{equation}\label{constrained-objective-function}
\hat{a}= \argmin_{a\in\mathbb{R}^N} \frac{1}{2} a^THa : \qquad B^Ta = \begin{pmatrix} -1 \\ 1 \end{pmatrix}
\end{equation} The solution $\hat{a}$ is determined uniquely by the choice of bandwidth $N$. Thus we will refer to the solution to the optimization problem with dimension $N$ as  \emph{the optimal coefficients vector $\hat{a}$ of degree $N$} and denote it $\hat{a}^{(N)}$. Sometimes when $N$ is understood we will omit it from notation. The vector $\hat{a}$ corresponds canonically to a polynomial $p_{\hat{a}} = \hat{a}(N) x^N + \cdots + \hat{a}(1) x$ and naturally to a odd-even polynomial pair $p_{e, \hat{a}}, p_{o, \hat{a}}$ $p_{e,\hat{a}}$ consists of the even degree monomials of $p_{\hat{a}}$ and $p_{o, \hat{a}}$ is $-1$ times the odd degree monomials of $p_{\hat{a}}$. \\
When necessary, we will refer to these polynomials as solutions of the optimization problem with a particular bandwidth. We will refer to the polynomials that are induced by $\hat{a}^(N)$ as the \emph{optimal polynomials or optimal polynomial pairs of degree $N$}, denoted as $p_{\hat{a}}^{(N)}$, $p_{\hat{a}_e}^{(N)}$, $p_{\hat{a}_o}^{(N)}$. 
We will show the following three properties: 
\begin{enumerate}
\item For every $N$, the entries of $\hat{a}^{(N))}$ are alternating in sign, with $\hat{a}(1) = -1$. This is equivalent to the fact that the optimal polynomial pairs have only positive coefficients.

\item As $N\to \infty$, our conjugacy loss objective function $\frac{1}{2} (\hat{a}^{(N)} )^T H \hat{a}^{(N)}\to 0$

\item For a fixed $k$, $\hat{a}^{(N)}(k) \to 0$ as $N\to \infty$. Thus the sequence of polynomials $\{p_{\hat{a}_e}^{(N)}\}$, $\{p_{\hat{a}_o}^{(N)}\}$ both converge pointwise (and in measure) to the indicator function at $1$.
\end{enumerate}
\end{prob}
But we may in fact solve (\ref{constrained-objective-function}) and obtain expressions for $\hat{a}$ via the method of Lagrange multipliers. Consider the generalized Lagrangian: 
\[ L(a,\lambda) = \frac{1}{2} a^T H a + \lambda^T (B^Ta - \begin{pmatrix} -1 \\ 1 \end{pmatrix} ) \]
Since the objective function associated with this Lagrangian is clearly convex and the equality condition is a hyperplane (affine), by KKT, a vector $a^*$ is a minimum of $f(a) = \frac{1}{2} a^T H a$ if and only if there exists a $\lambda^*$ such that together the pair $a^*, \lambda^*$ satisfy $\frac{\partial L(a^*,\lambda^*)}{\partial a} = \frac{\partial L(a^*,\lambda^*)}{\partial \lambda} = 0$. It will be more convenient to work with the alternate expression 
\[ L'(a,\lambda) = L(a,-\lambda) = \frac{1}{2} a^T H a - \lambda^T (B^Ta - \begin{pmatrix} -1 \\ 1 \end{pmatrix} ) \] 
Note that $\frac{\partial L'(a^*,\lambda^*)}{\partial a} = \frac{\partial L(a^*, -\lambda^*)}{\partial a}$, and $\frac{\partial L'(a^*,\lambda^*)}{\partial \lambda} = -\frac{\partial L(a^*, -\lambda^*)}{\partial \lambda}$, so if both partials of $L'$ are zero at $(a^*, \lambda^*)$, then both partials of $L$ are zero at $(a^*, -\lambda^*)$. Thus we must simply find the values of $\lambda, a$ such that the partials of $L'$ are zero. \\

Now, taking the derivative with respect to $a$, we see that we must have 
\[ \hat{a} = H^{-1} B \begin{pmatrix} \lambda_1 \\ \lambda_2 \end{pmatrix} = H^{-1} \begin{pmatrix} \lambda_1 \\ \lambda_2 \\ \vdots \\ \lambda_2 \end{pmatrix},\]
under the constraint 
\begin{align*}
&B^T\hat{a} = \begin{pmatrix} -1 \\ 1 \end{pmatrix} \\
\iff &B^T H^{-1} B \begin{pmatrix}\lambda_1 \\ \lambda_2 \end{pmatrix} = \begin{pmatrix} -1 \\ 1 \end{pmatrix}
\end{align*} With the constraint above in mind, the constrained objective function can now be rewritten as 
\begin{align*}
& \frac{1}{2}\hat{a}^T H \hat{a} = \frac{1}{2}a^T Ha \\
&= \frac{1}{2}(H^{-1}B \lambda)^TH(H^{-1}B \lambda) \\
&= \frac{1}{2} \lambda^T B^T H^{-1}B \lambda
\end{align*}
\section{Important Properties of the Generalized Hilbert Matrix}
\label{hilbert-matrices}
We see now that it will be useful to adopt some notation to refer to the inverse of the the Hilbert matrix, its entries and certain sums of collections of entries (such as rows or odd-indexed entries). Note, for example, that $H^{-1}B$ is a two column vector whose values are the sum of the odd or even-indexed entries in each row. 
\subsection{Definitions and Notations}
\begin{defn}
Let $G=H^{-1}$ where $H$ is a generalized Hilbert matrix of size $N$ for constant $p > 0$. Let $G_{i,j}$ denote the entries of $G$ (we write $G_{ij}$ when unambiguous). Furthermore, let 

\begin{align*} 
g^{oo} = \sum_{i,j \; \text{odd}}^N G_{i,j}, \qquad
g^{ee} = \sum_{i,j \; \text{even}}^N  G_{i,j}, \\
g^{oe} = \sum_{i \; \text{odd, } j \; \text{even}}^N  G_{i,j} ,\qquad
g^{eo} = \sum_{i \; \text{even, } j \; \text{odd}}^N  G_{i,j} 
\end{align*} be the sums of all entries of $G$ in, for example, odd-indexed rows and odd-indexed columns. Note in particular that since $H$ is symmetric, $G$ is also symmetric and therefore $g^{oe} = g^{eo}$ (when $G$ has even dimension). 

Now, let $1 \le i \le N$. Then define 
\begin{align*}
g_i^o= \sum_{j \; \text{odd}}^N G_{i, j} ,\qquad
g_i^e= \sum_{j \; \text{even}}^N G_{i, j} ,\qquad
r_i=g_i = \sum_{j=1}^N G_{i,j}
\end{align*} be sums of odd-indexed entries, the even-indexed entries, and all entries of a particular $i$-th row, respectively.\\
Finally define 
\begin{align*}
g^o = \sum_{i \; \text{odd}}^N r_i, \qquad 
g^e = \sum_{i \; \text{even}}^N r_i
\end{align*} be the sum of all entries in odd-numbered or even-numbered rows, respectively.
\end{defn} 
\begin{defn}\label{absolute-value-sum-of-entries}
For each definition above, we will adopt a notation to indicate its absolute value. We will let $s^o = |g^o|, s^e = |g^e|, s^e_k = |g^e_k|$, etc. In particular, note that $s_k^e + s_k^o$ is the sum of the absolute value of the entries of a particular row. These values will prove particularly useful.
\end{defn}

\subsection{Exact expression for entries and sum of entries}
\begin{prop}\label{hilbert-matrices-are-positive-definite}
Every generalized Hilbert matrix $H$ is positive definite. 
\end{prop}
\begin{proof} See \cite{collar1940reciprocation}.
\end{proof}

\begin{prop}\label{inverse-hilbert-matrix-entry}
Let $G=H^{-1}$ be the inverse of a generalized Hilbert matrix of size $N$ and constant $p>0$. Then the entries of $G$ are given by 
\begin{align}\label{entries}
 G_{ij} &= \frac{(-1)^{i+j}}{p+i+j-1} \left( \frac{\prod_{k=0}^{N-1} (p+i+k)(p+j+k)}{(i-1)! (N-i)! (j-1)! (N-j)!}\right) \\ 
 &= \label{entries-combinatorial-form} (-1)^{i+j} (p+i+j-1) \dbinom{N+p+i-1}{N-j} \dbinom{N+p+j-1}{N-i} \dbinom{p+i+j-2}{i-1} \dbinom{p+i+j-2}{j-1}
\end{align}
\end{prop}
\begin{proof} See \cite{collar1940reciprocation}.
\end{proof}

\begin{prop}\label{inverse-hilbert-matrix-row}
Let $G=H^{-1}$ be the inverse of a generalized Hilbert matrix of size $N$ and constant $p>0$. Fix some $i \in \{1,...,N\}$. Then the sum of the entries of row $i$ is given by: 
\begin{align}\label{sum-of-row}
\sum_{j=1}^N G_{ij} &= (-1)^{N+i} \frac{\prod_{k=0}^{N-1} (p+i+k)}{(i-1)! (N-i)!} \\
&= (-1)^{N+i} i \dbinom{N-1+p+i}{N} \dbinom{N}{i} \label{sum-of-row-combinatorial-form}
\end{align}
\end{prop}
\begin{proof} See \cite{smith1959inversion}.
\end{proof}

\begin{prop}\label{inverse-hilbert-matrix-total} 
Let $G=H^{-1}$ be the inverse of a generalized Hilbert matrix of size $N$ and constant $p>0$. Then the sum of entry of the matrix is given by:
\begin{equation}\label{total-sum}
\sum_{i,j=1}^N G_{ij} = N(p+N).
\end{equation}
\end{prop} 
\begin{proof} See \cite{smith1959inversion}.
\end{proof}

\begin{cor}\label{row-product} We obtain the following expression for products of rows:
\begin{align*}
r_ir_j &= (p+i+j-1) G_{ij} \\
&= (-1)^{i+j}(p+i+j-1)^2 \dbinom{N+p+i-1}{N-j} \dbinom{N+p+j-1}{N-i} \dbinom{p+i+j-2}{i-1} \dbinom{p+i+j-2}{j-1}
\end{align*}
\end{cor}

\begin{proof} The first equality follows quickly from comparing the expressions of (\ref{entries}), (\ref{sum-of-row}), and the second comes from applying the combinatorial form given in (\ref{entries-combinatorial-form}).
\end{proof}

\subsection{Notes on magnitudes and signs of important sums}
\begin{cor}
Let $N$ be even. Then for every $k$,  $|g_k^o| < |g_k^e|$. Furthermore $|g^{oo}| < |g^{oe}|$ and $|g^{eo}| < |g^{ee}|$.  If $N$ is odd then the inequalities are reversed. 
\end{cor}
\begin{proof}
This follows quickly from \ref{inverse-hilbert-matrix-row} and \ref{inverse-hilbert-matrix-entry}.
\end{proof}

\begin{cor}
Let $N$ be even. Then $g^o < 0 < g^e$. If $N$ is odd then the inequalities are reversed.
\end{cor}

\begin{prop}\label{inverse-determinant-is-positive}
The quantity $g^{oo}g^{ee} - g^{oe}g^{eo}$ is positive.
\end{prop}
\begin{proof}
Note that our quantity $g^{oo}g^{ee} - g^{oe}g^{eo}$ is the determinant of the matrix $B^TH^{-1}B = \begin{pmatrix} g^{oo} & g^{oe} \\ g^{eo} & g^{ee} \end{pmatrix}$. Furthermore, since $H$ is positive definite, $G=H^{-1}$ is positive definite. Thus, for any $x= \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} \in \mathbb{R}^2$, $x^T B^TGBx = (x_1 x_2 \cdots x_2) G \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_2 \end{pmatrix} > 0$ when $x$ is not identically zero. (More to the point, $B$ has full column rank). \\
Thus $B^TGB$ is positive definite and $\det B^TGB = g^{oo}g^{ee} - g^{oe}g^{eo}$ is positive. 
\end{proof}

\section{Theorems and Results}
\subsection{The positivity of optimal polynomial pair coefficients}

\begin{thm}\label{positivity-hilbert-coefficients}
Let $H$ be a generalized Hilbert matrix of size $N$ and $B \in M_{N\times 2}$ is as above. Furthermore, let 
\[ \hat{a} = \argmin_{a\in\mathbb{R}^N} \quad \quad a^THa : \quad B^Ta = \begin{pmatrix} -1 \\ 1 \end{pmatrix}\]
be the optimal vector of coefficients. Then for odd $i$, $\hat{a}(i) < 0$ and for even $i$ $\hat{a}(i) > 0$.
\end{thm} 

Our goal is to prove Theorem \ref{positivity-hilbert-coefficients}. In the succeeding theorems we show an equivalent result that we will prove. But first we will rewrite our expressions for the solution further. Recall that we have \[ \hat{a} = H^{-1} B \begin{pmatrix} \lambda_1 \\ \lambda_2 \end{pmatrix} = H^{-1} \begin{pmatrix} \lambda_1 \\ \lambda_2 \\ \vdots \\ \lambda_2 \end{pmatrix},\]
under the constraint 
\begin{align}
& B^T H^{-1} B \begin{pmatrix}\lambda_1 \\ \lambda_2 \end{pmatrix} = \begin{pmatrix} -1 \\ 1 \end{pmatrix} \\
\iff &  \begin{pmatrix}\lambda_1 \\ \lambda_2 \end{pmatrix}  = \frac{1}{g^{oo}g^{ee}-g^{oe}g^{eo}} \begin{pmatrix} g^{ee} & -g^{oe} \\ -g^{eo} & g^{oo} \end{pmatrix} \begin{pmatrix} -1 \\ 1 \end{pmatrix} \\
\iff & \begin{pmatrix}\lambda_1 \\ \lambda_2 \end{pmatrix}  = \frac{1}{g^{oo}g^{ee}-g^{oe}g^{eo}} \begin{pmatrix} -g^{ee} - g^{oe} \\ g^{eo} + g^{oo} \end{pmatrix} =\frac{1}{g^{oo}g^{ee}-g^{oe}g^{eo}} \begin{pmatrix} -g^e \\ g^o \end{pmatrix} \label{lambda-expression}
\end{align}

\begin{prop}\label{ratio-inequality-is-equivalent-to-main-theorem}
Let $s^o, s^e, s^o_k, s^e_k$ as in Definition \ref{absolute-value-sum-of-entries}. Furthermore, if $N$ is even, let $s^+ = s^o + s^e$, $s^- = s^e-s^o$, and for any $i\in \{1,...,N\}$, let $s_k^+ = s_k^o + s_k^e$ and $s_k^- = s_k^e- s_k^o$. Otherwise, let $s^- = s^o - s^e, s_k^- = s_k^o - s_k^e$. Then: 
\begin{enumerate}
\item \begin{equation}\label{ratio-of-absolute-difference-inequality}
\frac{s^+}{s^-} < \frac{s_k^+}{s_k^-} 
\end{equation}

\item Inequality \ref{ratio-of-absolute-difference-inequality} implies our Theorem \ref{positivity-hilbert-coefficients}.
\end{enumerate}
\end{prop}
Note that our $s_k^+$ are simply defined to be the of the sum of the absolute of even-indexed and odd-indexed values, and the $s_k^-$ to be the absolute value of the sum of the even-indexed and odd-indexed values, respectively. Both $s^-$ and $s_k^-$ are familiar values - modulo an absolute value sign, they are simply the sum of entries in a row, and the total sum of all elements. The $s^+, s_k^+$ values are meant to be an absolute value analog, achieving far larger magnitudes. The point of Proposition \ref{ratio-inequality-is-equivalent-to-main-theorem} is to compare ratios of these magnitudes and relate them directly to the coefficients of our optimal vector $\hat{a}$.
\begin{proof}
We will first show that Inequality \ref{ratio-of-absolute-difference-inequality} implies our Theorem \ref{positivity-hilbert-coefficients}.\\
First, consider even $N$. Note 
\begin{align*}
& \frac{s^+}{s^-} < \frac{s_k^+}{s_k^-}\\
\iff & \frac{s^-}{s^+} > \frac{s^-_k}{s^+_k}\\
\iff& \frac{1-s^-/s^+}{1+s^-/s^+} < \frac{1-s^-_k /s^+_k}{1+s^-_k/s^+_k} \\
\iff & \frac{s^+ - s^-}{s^+ +s^-} <\frac{s^+_k - s^-_k}{s^+_k +s^-_k} \\
\iff&\frac{s^o}{s^e} < \frac{s^o_k}{s^e_k} 
\end{align*} where we have taken advantage of the fact that $0<\frac{s^-}{s^+}, \frac{s^-_k}{s^+_k}<1$. The inequality at the end is reversed for odd $N$. Now, let $k <N$ be the index of any row. Then the expressions $\frac{g^o}{g^e}, \frac{g^o_k}{g^e_k}$ must contain exactly one negative entry in each fraction, and thus our inequality $\frac{s^o}{s^e} < \frac{s^o_k}{s^e_k}$ is equivalent to $\frac{g^o}{g^e} > \frac{g^o_k}{g^e_k}$. (The inequality is reversed for odd $N$). If $N$ is even, then $g^e$ is positive; otherwise $g^o$ is negative. In either case, if $k$ is even, our previous inequality is equivalent to 
\begin{align}
&g^og^e_k > g^eg^o_k \\
\iff & (g^{oo}g^{ee} - g^{oe}g^{eo})^{-1}(-g^eg^o_k+g^og^e_k)>0 \\
\iff& \lambda_1 g^o_k + \lambda_2 g^e_k >0 \label{even-lambda-inequality}
\end{align}
by (\ref{lambda-expression}). If $k$ is odd, this inequality is 
\begin{equation}\label{odd-lambda-inequality} \lambda_1 g^o_k + \lambda_2 g^e_k <0 \end{equation}
But $B \begin{pmatrix} \lambda_1 \\ \lambda_2 \end{pmatrix} = \begin{pmatrix} \lambda_1 \\ \lambda_2 \\ \vdots \\ \lambda_2 \end{pmatrix}$, so the $k$th element of $a$ is given by 
\begin{equation}\label{ratio-linear-combination} \lambda_1 g_k^o + \lambda_2 g_k^e \end{equation} so in fact our two inequalities (\ref{even-lambda-inequality}), (\ref{odd-lambda-inequality}) together give the statement of our main Theorem \ref{positivity-hilbert-coefficients}.

Now onto the inequality. We can rewrite the inequality (\ref{ratio-of-absolute-difference-inequality}) as: 
\begin{align*}
&\frac{s^+}{s^-} < \frac{s_i^+}{s_i^-} \\
\iff & \frac{\sum_{i=1}^N |r_i|}{\sum_{i=1}^N r_i} < \frac{\sum_{j=1}^N |G_{kj}|}{\left | \sum_{j=1}^N G_{kj} \right|}\\
\iff & \frac{\sum_{i=1}^N |r_i |}{ N(p+N)} < \frac{\sum_{j=1}^N |G_{kj}|}{|r_k|}\\
\iff & |r_k| \sum_{i=1}^N |r_i| < N(p+N) \sum_{j=1}^N |G_{kj}|\\
\iff & \sum_{i=1}^N (p+i+j-1) |G_{kj}| < N(p+N) \sum_{j=1}^N |G_{kj}|
\end{align*} But $p+i+j-1 < 2N-1+p$. $N^2 > 2N-1$ when $N$ is at least 2, and $Np>p$ when $N>1$, and thus we see that our theorem is proven.
% begin loooooooonnggggg comment
\begin{comment}We write down two lemmas on binomial coefficients that will prove useful. Both are well-known and easily verified.
\begin{lem}\label{absorption-combination}
Let $N>j>0$. Then 
\[ j \dbinom{N}{j} = j \dbinom{N}{N-j} = N \dbinom{N-1}{j-1}\]
\end{lem} 

\begin{lem}\label{subset-of-a-subset-combination}
\[ \dbinom{N}{m} \dbinom{m}{k} = \dbinom{N}{k} \dbinom{N-k}{m-k} \]
\end{lem} 
\bigskip


Note that in our expressions for the individual entries or sums of rows of $G$, there is usually a term like $(-1)^{i+j}$ dictating the sign of the value. Since we wish to compare a ratio of sums after taking an absolute value, we can in fact ignore the sign term. Thus plugging in our values from our propositions into the inequality \ref{ratio-of-absolute-difference-inequality}, we see that our inequality is equivalent to 

\begin{equation}\label{inequality-all-expressions}
\frac{\sum_{i=1}^N i \dbinom{N-1+p+i}{N} \dbinom{N}{i}}{N(P+N)} < \frac{\sum_{j=1}^N (p+k+j-1) \dbinom{N+p+k-1}{N-j} \dbinom{N+p+j-1}{N-k} \dbinom{p+k+j-2}{k-1}^2 }{k \dbinom{N-1+p+k}{N}\dbinom{N}{k}} 
\end{equation}  for every $k\in \{ 1,...,N\}$.\\

Let us first consider the LHS. Using Proposition \ref{absorption-combination}, we see that 
\begin{align}
\frac{\sum_{i=1}^N i \dbinom{N-1+p+i}{N} \dbinom{N}{i}}{N(P+N)}  &= \frac{\sum_{i=1}^N N \dbinom{N-1+p+i}{N}  \dbinom{N-1}{i-1}}{N(P+N)} \\
&= \frac{\sum_{i=1}^N\dbinom{N-1+p+i}{N}  \dbinom{N-1}{i-1}}{P+N} \\
&< \frac{\sum_{i=1}^N\dbinom{N-1+p+i}{N}  \dbinom{N}{i}}{P+N} \label{LHS-upper-bound}
\end{align}

Now consider the RHS. The denominator can be rewritten as 
\begin{align*} 
k \dbinom{N-1+p+k}{N}\dbinom{N}{k} &= N \dbinom{N-1+p+k}{N}\dbinom{N-1}{k-1} \\ 
&= (N+p+k-1) \dbinom{N+p+k-2}{N-1} \dbinom{N-1}{k-1} 
\end{align*} and plugging into the RHS and expanding gives 
\begin{align*}
&\frac{\sum_{j=1}^N (p+k+j-1) \dbinom{N+p+k-1}{N-j} \dbinom{N+p+j-1}{N-k} \dbinom{p+k+j-2}{k-1} \dbinom{p+k+j-2}{j-1} }{ (N+p+k-1) \dbinom{N+p+k-2}{N-1} \dbinom{N-1}{k-1} }  \\
&= \frac{ (N+p+k-1)\sum_{j=1}^N  \dbinom{N+p+k-2}{p+k+j-2} \dbinom{N+p+j-1}{N-k} \dbinom{p+k+j-2}{k-1} \dbinom{p+k+j-2}{j-1} }{ (N+p+k-1) \dbinom{N+p+k-2}{N-1} \dbinom{N-1}{k-1} }  \\
&= \frac{\sum_{j=1}^N \frac{(N+p+k-2)!}{(p+k+j-2)!(N-j)!} \frac{(N+p+j-1)!}{(N-k)! (p+j+k-1)!} \left(\frac{(p+k+j-2)!^2}{(k-1)! (p+j-1)!(j-1)!(p+k-1)!}\right)}{\frac{(N+p+k-2)!}{(N-1)! (p+k-1)!} \frac{(N-1)!}{(k-1)! (N-k)!}}
\\
&= \sum_{j=1}^N \frac{(N+p+j-1)! (p+k+j-2)!}{(N-j)!(p+j+k-1)!(p+j-1)!(j-1)!}\\
&= \sum_{j=1}^N \frac{(N+p+j-1)!N! j}{(p+j-1)!N!(N-j)!j! (p+j+k-1)}\\
&= \sum_{j=1}^N \dbinom{N+p+j-1}{N} \dbinom{N}{j} \frac{j}{p+j+k-1}
\end{align*} which is clearly monotonically decreasing in $k$, so it suffices to consider 
\begin{align*}
\sum_{j=1}^N \dbinom{N+p+j-1}{N} \dbinom{N}{j} \frac{j}{p+j+N-1} > \sum_{j=1}^N \dbinom{N+p+j-1}{N} \dbinom{N}{j}\frac{1}{p+N}
\end{align*} since the fraction $\frac{j}{p+j+N-1}$ is increasing in $j$ because both $j$ and $p+N-1$ positive.
\end{comment}
% end loooooooonnggggg comment
\end{proof}


\subsection{Asymptotic behavior of polynomial pairs}
We now show that not only does this optimization of quasi-conjugate polynomial pairs give us well-behaved polynomials with positive coefficients, but also that our quadrature loss vanishes as the maximal degree allowed $N$ is increased. 

\begin{thm}
Let $\hat{a}^{(N)}$ denote the optimal vector of coefficients pertaining to the optimization problem of dimension $N$. Then the objective function loss $\frac{1}{2}(\hat{a}^{(N)})^T H \hat{a}^{(N)} \to 0$ as $N\to \infty$.
\end{thm}
\begin{proof}
Let us fix some $N$ and simply write $\hat{a}$ as the corresponding solution. We proceed by deriving an alternative expression for the objective function using our previous results and then prove some facts about the growth. Recall
\begin{align*}
\frac{1}{2}a^T Ha &=\frac{1}{2} \lambda^T B^T H^{-1}B \lambda \\
&= \frac{1}{2} \lambda^T K\lambda, \quad \text{where } K= \begin{pmatrix} g^{oo} & g^{oe} \\
g^{eo} & g^{ee} \end{pmatrix}.
\end{align*} But $\lambda = K^{-1} \begin{pmatrix} -1 \\ 1 \end{pmatrix} = \lambda_G \begin{pmatrix} -g^e \\ g^o \end{pmatrix}$, where $\lambda_G = \frac{1}{g^{oo} g^{ee} - g^{eo}g^{oe}} >0$, so noting the symmetry of $K$, we have 
\begin{align*}
\frac{1}{2} a^T Ha &= \frac{1}{2}\begin{pmatrix} -1 \\ 1\end{pmatrix} ^T K^{-1} K \lambda \\ 
&= \frac{\lambda_G}{2}(-1 \; 1) \begin{pmatrix} -g^e \\ g^o \end{pmatrix}\\
&= \frac{g^e + g^o}{2(g^{oo} g^{ee} - g^{eo}g^{oe})}\\
&= \frac{N(p+N)}{2(g^{oo}g^{ee} - (g^{oe})^2)}
\end{align*}
Now, let $d=|g^{oe}| - |g^{oo}|$. Then since $N(p+N) = g^{ee} + g^{oo} + g^{oe} + g^{eo} = g^{oo}+g^{ee} - 2|g^{oe}|$, we may write $g^{oo} = |g^{oe}|-d$, $g^{ee} = |g^{oe}| + d + N(P+N)$. Thus 
\begin{align*}
g^{oo} g^{ee} - g^{eo}g^{oe} &=  (|g^{oe}| + d + N(p+N)) (|g^{oe}| - d) - |g^{oe}|^2 \\
&= (|g^{oe}|+d)(|g^{oe}| - d) + N(p+N)(|g^{oo}|) - |g^{oe}|^2\\
&= N(p+N)(g^{oo}) - d^2
\end{align*}
We now examine the growth of $g^{oo}$ and $d^2$. $d$ is simply a sign change times the sum of the odd rows. We have
\[d = (-1)^N \sum_{i \text{ odd}}^N \frac{\prod_{k=0}^{N-1} (p+i+k)}{(i-1)!(N-i)!} \] 
and thus we may write 
\[d^2 = \sum_{i,j \text{ odd}}^N 
\frac{\prod_{k=0}^{N-1} (p+i+k)(p+j+k)}{(i-1)! (N-i)! (j-1)! (N-j)!} \]

On the other hand, using the fact that $g^{oo}$ is a sum of only positive entries we can write 
\[g^{oo} = \sum_{i,j \text{ odd}}^N \frac{1}{p+i+j-1} \frac{\prod_{k=0}^{N-1} (p+i+k)(p+j+k)}{(i-1)! (N-i)! (j-1)! (N-j)!}\]
So we can write the denominator of our objective function value as

\begin{align*} 2(g^{oo} g^{ee}-(g^{oe})^2) &= 2(N(p+N)(g^{oo}) - d^2)\\ &= \sum_{i,j \text{ odd}}^N \left( \frac{2N(p+N)}{p+i+j-1} - 1\right) \frac{\prod_{k=0}^{N-1} (p+i+k)(p+j+k)}{(i-1)! (N-i)! (j-1)! (N-j)!} \\
& \ge \sum_{i,j \text{ odd}}^N (N-1) \frac{\prod_{k=0}^{N-1} (p+i+k)(p+j+k)}{(i-1)! (N-i)! (j-1)! (N-j)!}\\
&\ge \sum_{i,j \text{ odd}}^N (N-1) \frac{(N-1)!^2}{(i-1)! (N-i)! (j-1)! (N-j)!} \\
&= \sum_{i,j \text{ odd}}^N (N-1) \dbinom{N-1}{i-1} \dbinom{N-1}{j-1}\\
&= (N-1) \left(\sum_{i' \text{even}}^{N-1} \dbinom{N-1}{i'}\right) \left( \sum_{j' \text{even}}^{N-1} \dbinom{N-1}{j'}\right)\\
&= (N-1)(2^{N-2})(2^{N-2}) = (N-1)2^{2N-4}
\end{align*} 
Since our numerator grows like $O(N^2)$, we are done, we see clearly that our fraction vanishes as $N \to \infty$. \end{proof}

Note that for a fixed degree, any polynomial with positive coefficients that add up to 1, the function is bounded from below by $x^N$ for every $x$. We will prove a proposition that states, essentially, that as we increase the maximum degree allowed during our quadrature-loss optimization, the polynomials we obtain the in the pair begin to resemble a delta function at 1 - that is, the polynomials converge pointwise to 0 (except at $x=1$). We will require two lemmas. 

The first lemma is a computational lemma that will make the proof of the second lemma, one on the behavior of the $k$th coefficient of $\hat{a}$, easier to prove. We have
\begin{lem}\label{computational-inequality-lemma}
Let $N$ be sufficiently large - say, larger than both $p$ and $2$, and fix $j \le N$. Then the expression:
\begin{equation} \label{weighted-entries-increasing-expression}(N(p+N)-(p+i+j-1))|G_{ij}|\end{equation} is increasing in $i$ on the interval $[1,N/2]$.
\end{lem}
\begin{proof}
We first expand (\ref{weighted-entries-increasing-expression}) as
\[(N(p+N)-(p+i+j-1))(p+i+j-1)\dbinom{N+p+i-1}{N-j} \dbinom{N+p+j-1}{N-i} \dbinom{p+i+j-2}{i-1} \dbinom{p+i+j-2}{j-1}\]
Expanding and collecting terms that do not depend on $i$ in a positive coefficient $C(N,j,p)$, we consider 
\begin{align*}
&C(N,j,p)(N(p+N)-(p+i+j-1))(p+i+j-1) \frac{(N+p+i-1)!(p+i+j-2)!^2}{(p+i+j-1)!^2(N-i)!(i-1)!(p+i-1)!}\\
&= C(N,j,p)(N(p+N)-(p+i+j-1))\frac{(N+p+i-1)!(p+i+j-2)!(N-1)!}{(p+i+j-1)!(p+i-1)!(N-i)!(i-1)!}\\
&= C'(N,j,p)(N(p+N)-(p+i+j-1))\frac{(N+p+i-1)!(p+i+j-2)!}{(p+i+j-1)!(p+i-1)!}\dbinom{N-1}{i-1}
\end{align*} Now, $\dbinom{N-1}{i-1}$ is increasing in $i$ when $i-1 < (N-1)/2 \iff i < \frac{N+1}{2}$. Thus our rightmost factor increases with $i$ on our interval [1,N/2]. Now, we consider the remaining factors depending on $i$ as a function of $i$ (regarding $N, j$ as constants):
\[f(i) = (N(p+N)-(p+i+j-1))\frac{(N+p+i-1)!(p+i+j-2)!}{(p+i+j-1)!(p+i-1)!}\]
Then, we may compute:
\begin{align*}
\frac{f(i+1)}{f(i)} &= \frac{(N(p+N)-(p+i+j))\frac{(N+p+i)!(p+i+j-1)!}{(p+i+j)!(p+i)!}}{(N(p+N)-(p+i+j-1))\frac{(N+p+i-1)!(p+i+j-2)!}{(p+i+j-1)!(p+i-1)!}} \\
&= \frac{(N(p+N)-(p+i+j))}{(N(p+N)-(p+i+j-1))} \frac{(N+p+i)(p+i+j-1)}{(p+i+j)(p+i)}\
\end{align*}
Since $1\le j \le N$, for at least one of $\frac{N+p+i}{j+p+i}, \frac{p+i+j-1}{p+i}$, the numerator is greater than the denominator by at least one (and they are both at least 1). Now, $N(p+N)-(p+i+j-1) > p+i+j \iff N^2 +Np>2p+2i+2j-1$. But $2i+2j-1<4N$, so if $N>4$ then $N^2+Np>4N+4p>2i+2j-1+2p$. Similarly, $N(p+N)-(p+i+j-1)>p+i$ under weak conditions on $N$, and thus letting $A=N(p+N)-(p+i+j)$ and $B<A$, we have 
\[\frac{f(i+1)}{f(i)} > \frac{A}{A+1} \frac{B+1}{B} > 1\]
as needed.
\end{proof} 

Now we prove a lemma on the coefficients of our optimally obtained vector $\hat{a}$.
\begin{lem}\label{coefficients-vanish}
Let $\hat{a}^{(N)}$ denote the optimal vector of coefficients of dimension $N$, and $\hat{a}^{(N)}(k)$ its $k$-th coefficient. Now, fix $k$. Then, $\hat{a}^{(N)}(k) \to 0$ as $N\to \infty$.
\end{lem}
This lemma states that any fixed coefficient goes to 0 in our sequence of optimal vectors $\hat{a}$. We will exploit the fact that $x^m$ decreases with $m$ when $0<x<1$ to use this lemma in our main proposition. 
\begin{proof}
Recall from earlier that $a_k$ is given by:
\begin{align}
a_{k} &= \lambda_1 g_k^o + \lambda_2g^e_k \\
&= \lambda_G(-g^eg_k^o + g^og_k^e) \\
&= \frac{-g^eg_k^o + g^og_k^e}{g^{oo} g^{ee}-(g^{oe})^2}\label{coefficient-ratio-form}
\end{align}
Recall that we may write our denominator
\[g^{oo} g^{ee}-(g^{oe})^2 = N(p+N)g^{oo} - d^2 \]
where $d$ was the absolute value of the sums of all elements in all odd rows. Using Corollary \ref{row-product} we may rewrite the denominator of \ref{coefficient-ratio-form} as:
\begin{align*}
g^{oo} g^{ee}-(g^{oe})^2 &= N(p+N) \sum_{i,j \; \text{odd}} G_{ij} - \sum_{i,j \; \text{odd}} r_ir_j \\
&= N(p+N) \sum_{i,j \; \text{odd}} G_{ij} - \sum_{i,j \; \text{odd}} (p+i+j-1) G_{ij}\\
&= \sum_{i,j \; \text{odd}}  (N(p+N) -(p+i+j-1))G_{ij}
\end{align*}
Now consider the numerator of  \ref{coefficient-ratio-form}. Noting that $g_k^e = -g_k^o + r_k$, we can rewrite our numerator as: 
\begin{align*}
-g^eg_k^o + g^og_k^e &= g^e(-g^o_k) + g^o(-g^o_k + r_k)\\
&= -g^o_k(g^e+g^o) + g^o r_k\\
&= -(N(p+N))g_k^o + g^or_k\\
&= -(N(p+N)\sum_{j \; \text{odd}} G_{kj} + \left(\sum_{j \; \text{odd}}r_j \right)r_k \\
&= \sum_{j\; \text{odd}} (p+k+j-1 - N(p+N))G_{kj}.
\end{align*}
Now, the sign of summands is dependent entirely on $k$. Since $j$ ranges over odd values, $G_{kj}$ is negative is $k$ is even, and positive when $k$ is odd. But the summands have the same sign, so since we are concerned only with magnitude we may flip the sign for convenience. In other words it suffices to show that
\begin{align}\label{coefficient-expanded-ratio}
&(-1)^k\frac{-g^eg_k^o + g^og_k^e}{g^{oo} g^{ee}-(g^{oe})^2} = \frac{ \sum_{j\; \text{odd}} (N(p+N) -(p+k+j-1))G_{kj}}{ \sum_{i,j \; \text{odd}}  (N(p+N) -(p+i+j-1))G_{ij}}
\end{align}
decays to 0 as $N\to \infty$. Take $N>4k$, and let $K = \{i \; \text{odd}: N/4  \le i \le N/2\}$. First, note that this implies $N>4$, and that $|K|$ is approximately $N/8$. Certainly, for large $N$ we may say that $|K|>N/16$. Furthermore, since $k<N/4$, by Lemma \ref{computational-inequality-lemma}, we have that for any $i\in K$, 
\begin{equation}\label{inequality-sum-of-products} \left|\sum_{j\; \text{odd}} (p+i+j-1 - N(p+N))G_{ij} \right| > \left| \sum_{j\; \text{odd}} (p+k+j-1 - N(p+N))G_{kj} \right|\end{equation}
Thus from (\ref{coefficient-expanded-ratio}), we may write
\begin{align*}
\frac{ \sum_{j\; \text{odd}} (N(p+N) -(p+k+j-1))G_{kj}}{ \sum_{i,j \; \text{odd}}  (N(p+N) -(p+i+j-1))G_{ij}} &< \frac{ \sum_{j\; \text{odd}} (N(p+N) -(p+k+j-1))G_{kj}}{ \sum_{j\; \text{odd, }\; i\in K}  (N(p+N) -(p+i+j-1))G_{ij}}\\
& < \frac{ \sum_{j\; \text{odd}} (N(p+N) -(p+k+j-1))G_{kj}}{\frac{N}{16} \sum_{j \; \text{odd}}  (N(p+N) -(p+k+j-1))G_{kj}} = \frac{16}{N}
\end{align*} which clearly decays to 0 as $N\to \infty$.
\end{proof}

\begin{prop}\label{pointwise-convergence-zero-one-function}
Let $\hat{p_N}$ be the optimal polynomial derived from the optimization problem in \ref{quasiconjugacy-poly-formulation} of dimension $N$. Let $x\in (0,1)$. Then $p_N(x) \to 0$ as $N\to \infty$. 
\end{prop}
This proposition says that as we increase the bandwidth of our polynomials, the optimal pairs converge to a delta function at 1.

\begin{proof}
Let $\epsilon > 0$. 
First, there is some $M$ such that $x^M < \frac{\epsilon}{4}$. \\
Recall that any $\hat{a}^{(N)}$ corresponds to a pair of polynomial with coefficients $\hat{a}^{(N)}(k)$. It suffices to show that the polynomial given by $p_N(x) = \sum_{k=1}^N |\hat{a}^{(N)}(k)| x^k$ converges pointwise to 0 at every $x\neq 1$, since the optimal odd/even polynomials are bounded by this polynomial. By Lemma \ref{coefficients-vanish}, there is some $L$ such that for every $N>\max(M,L)$, $\hat{a}^(N)$ satisfies $\max (a_{N,1}, a_{N,2},..., a_{N,M-1}) < \frac{\epsilon(1-x)}{2x}$. Selecting such a $M, N$, let $c = \max (a_{N,1}, a_{N,2},..., a_{N,M-1}) $.\\
Now, using the facts above and the fact that $\sum_{k=1}^N |\hat{a}^{(N)}(k)| = 2$,
\begin{align*}
p_N(x) &= \sum_{k=1}^{M-1} a_{N,k} x^k + \sum_{k=M}^N a_{N,k}x ^k\\
&< c \sum_{k=1}^{M-1} x^ k + \sum_{k=M}^N a_{N,k} x^M \\
&< \frac{\epsilon(1-x)}{2} \sum_{k=1}^\infty x^k + x^M \sum_{k=M}^N a_{N,k}\\
& < \frac{\epsilon(1-x)}{2x} \frac{x}{1-x} + 2x^M < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
\end{align*}
as needed.
\end{proof}


\bibliographystyle{unsrt}  
\bibliography{references}  



\end{document}